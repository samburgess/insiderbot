At this stage we have parsed all our needed data and have begun constructing our models. The data parsing involved creating a dataset of a few hundred known cases of insider trading of the form <ticker, press release date> where the press release date is the date that the company who's stock was being fraudulently traded made the announcement that the insider trading was in advance of. We then cross reference this dataset with a massive dataset of United States Stock markets form kaggle to create 30 day windows, ending a few days after the announcement. We refer to this set as our 'positive data' - cases where we know insider  trading was occuring on a significant scale. We then parsed similar data for 'negative data' - windows where we assume no insider trading was occuring. We are taking large scale insider trading to be a rare enough event that we can randomly grab windows from stock data and assume no significant fraudulent insider trading was occuring. We got our negative windows by scanning the entire kaggle dataset for Stock volume spikes that are similar to the spikes that happen after an announcement in our fraudulent cases, then creating a set of windows of the same structure to our positive data. 

We are using cross-validation instead of a dedicated dev set due to the small number of positive cases we were able to obtain in the time available (a few hundred). This week we will have created a price prediction machine which we will use to augment our input to our main insider trading / no insider trading classifier. The augmented field will be the difference between the predicted price, volume and other measurements and the actual. This should allow us to blend anomoly detection and classification approaches. I got the idea from a paper  about detecting credit card fraud using a mixture of unsupervised and supervised prediction that the researchers kindly sent me.

